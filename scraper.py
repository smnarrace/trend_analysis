from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

def setup_driver():
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    options.add_argument('--window-size=1920,1080')
    options.binary_location = '/usr/bin/google-chrome'
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def get_munpia(driver):
    driver.get("https://www.munpia.com/page/best/section/real")
    time.sleep(4) 
    
    # ğŸ’¡ [ì—‘ìŠ¤ë ˆì´] ì§€ê¸ˆ ë´‡ì´ ë¬´ìŠ¨ í™”ë©´ì„ ë³´ê³  ìˆëŠ”ì§€ í™•ì¸!
    print(f"   [X-Ray] ë¬¸í”¼ì•„ í™”ë©´ ì œëª©: {driver.title}") 
    
    titles_set = set()
    for _ in range(5):
        driver.execute_script("window.scrollBy(0, 1000);")
        time.sleep(1)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ğŸ’¡ [íˆ¬ë§ ë˜ì§€ê¸°] í´ë˜ìŠ¤ëª… ë¬´ì‹œí•˜ê³  ëª¨ë“  <a> íƒœê·¸ì˜ ê¸€ì”¨ë¥¼ ë‹¤ ê¸ì–´ì˜µë‹ˆë‹¤!
        for a in soup.find_all('a'):
            text = a.get_text(strip=True)
            if len(text) > 3 and '@' not in text: # 4ê¸€ì ì´ìƒë§Œ ë‹´ê¸°
                titles_set.add(text)
    return list(titles_set)

def get_naver(driver):
    driver.get("https://comic.naver.com/webtoon")
    time.sleep(3) 
    print(f"   [X-Ray] ë„¤ì´ë²„ í™”ë©´ ì œëª©: {driver.title}")
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    titles = [t.get_text(strip=True) for t in soup.select('.ContentTitle__title--e3qXt')]
    if not titles:
        titles = [t.get_text(strip=True) for t in soup.find_all('span', class_='title')]
    return titles

def get_kakao(driver):
    driver.get("https://page.kakao.com/menu/10011/screen/94")
    time.sleep(5) 
    
    print(f"   [X-Ray] ì¹´ì¹´ì˜¤ í™”ë©´ ì œëª©: {driver.title}")
    
    titles_set = set()
    for _ in range(8):
        driver.execute_script("window.scrollBy(0, 800);") 
        time.sleep(1)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ğŸ’¡ [íˆ¬ë§ ë˜ì§€ê¸°] <span>ê³¼ <div>ì— ìˆëŠ” ëª¨ë“  ê¸€ì”¨ë¥¼ ë¬´ì‹í•˜ê²Œ ë‹¤ ê¸ì–´ì˜µë‹ˆë‹¤!
        for tag in soup.find_all(['span', 'div']):
            text = tag.get_text(strip=True)
            if len(text) > 4: # 5ê¸€ì ì´ìƒë§Œ ë‹´ê¸°
                titles_set.add(text)
    return list(titles_set)

def scrape_all_to_csv(filename="webnovel_raw.csv"):
    print("ğŸš€ ë¬´ì‹í•œ íˆ¬ë§ í¬ë¡¤ëŸ¬ ì¶œë™!")
    driver = setup_driver()
    all_data = []
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    platforms = {'Munpia': get_munpia, 'Naver': get_naver, 'Kakao': get_kakao}

    try:
        for name, func in platforms.items():
            print(f"\n[{name} ìˆ˜ì§‘ ì‹œì‘]")
            try:
                titles = func(driver)
                print(f"-> {name} ë°ì´í„° {len(titles)}ê±´ í™•ë³´!")
                for title in titles:
                    all_data.append([current_time, name, title])
            except Exception as e:
                print(f"-> ì—ëŸ¬: {e}")
    finally:
        driver.quit() 

    df = pd.DataFrame(all_data, columns=['collected_at', 'platform', 'title'])
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"\n--- ğŸ‰ ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {filename} (ì´ {len(df)}ê±´) ---")

if __name__ == "__main__":
    scrape_all_to_csv()